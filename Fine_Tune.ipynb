{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J6GK0FG8KWTA"
      },
      "outputs": [],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install transformers datasets accelerate peft trl bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piZWTuc6KYIF"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "MAX_SEQ_LEN = 2048\n",
        "LOAD_IN_4BIT = True\n",
        "# MODEL_NAME = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
        "MODEL_NAME = \"unsloth/gemma-7b-it-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        "    dtype=None,\n",
        "    load_in_4bit=LOAD_IN_4BIT,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaY0BqCrLkpx"
      },
      "source": [
        "# load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1tZ_tIULs5a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX6BqgRtLt9k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_excel('/content/drive/MyDrive/datasets/News.xlsx')\n",
        "dataset.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lnh_lgqLyQ4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = dataset.drop(columns=['Unnamed: 0'])\n",
        "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjNnvxc1LQ12"
      },
      "source": [
        "# Zero shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA2yHR9YLLfX"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNrAv8ua9U7u"
      },
      "outputs": [],
      "source": [
        "max_model_tokens = 1998\n",
        "max_new_tokens = 50\n",
        "\n",
        "def truncate_prompt(tokenizer, text, max_model_tokens=max_model_tokens, max_new_tokens=max_new_tokens):\n",
        "    # تبدیل متن به توکن\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "    # اگر طول ورودی زیاد است، فقط آخر متن را نگه دار\n",
        "    if input_ids.shape[-1] + max_new_tokens > max_model_tokens:\n",
        "        truncated_ids = input_ids[:, -(max_model_tokens - max_new_tokens):]\n",
        "        text = tokenizer.decode(truncated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVlPHqdh7Ve9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_output_title(text):\n",
        "    match = re.split(r\"### title:\", text, flags=re.IGNORECASE)\n",
        "    if len(match) > 1:\n",
        "        return match[-1].strip()\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "b-LU1qO9NVON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zero shot fo i-th of test dataset\n",
        "def zero_shot(i):\n",
        "  print(\"real title: \" + test_data['title'].iloc[i])\n",
        "  content = test_data['content'].iloc[i]\n",
        "\n",
        "  prompt1 = f\"\"\"The text below is a news content. Your task is to write a short, clear, and relevant Persian title that summarizes the main idea of the article.\n",
        "  Generate only one Persion title and do not include any translations or extra text.  News content is:\\n\\n\"\n",
        "              \"{content}\\n\\ntitle:\"\"\"\n",
        "\n",
        "  result = pipe(prompt1, max_new_tokens=50, temperature=0.6, do_sample=True)\n",
        "  print(\"Result for prompt 1: \" + clean_output_title(result[0][\"generated_text\"]))\n",
        "\n",
        "  prompt2 = f\"\"\"Write a title in Persian for the news article below.\n",
        "  Do not include any English words, translations, or extra explanations. News content is:\\n\\n\"\n",
        "              \"{content}\\n\\ntitle:\"\"\"\n",
        "\n",
        "  result = pipe(prompt2, max_new_tokens=50, temperature=0.6, do_sample=True)\n",
        "  print(\"Result for prompt 2: \" + clean_output_title(result[0][\"generated_text\"]))\n",
        "\n",
        "  prompt3 = f\"\"\"You are a journalist. Your task is to read the news content below and write a single, clear, and concise title **in Persian** that best summarizes the main idea.\n",
        "  Do not include any English words, translations, or extra text. News content is:\\n\\n\"\n",
        "              \"{content}\\n\\ntitle:\"\"\"\n",
        "\n",
        "  result = pipe(prompt3, max_new_tokens=50, temperature=0.6, do_sample=True)\n",
        "  print(\"Result for prompt 3: \" + clean_output_title(result[0][\"generated_text\"]))\n",
        "\n",
        "zero_shot(0)\n",
        "zero_shot(1)\n"
      ],
      "metadata": {
        "id": "g5LiiM_fMekh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kYFvlEFwf0R"
      },
      "source": [
        "# Few shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjvtQXYZ7VbT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VPSyMKT5B-s"
      },
      "outputs": [],
      "source": [
        "\n",
        "example_1 = f\"News content:\\n{train_data['content'].iloc[0]}\\nTitle:\\n{train_data['title'].iloc[0]}\"\n",
        "example_2 = f\"News content:\\n{train_data['content'].iloc[1]}\\nTitle:\\n{train_data['title'].iloc[1]}\"\n",
        "\n",
        "examples_text = f\"{example_1}\\n\\n{example_2}\\n\\nNow write the title for the next news:\\n\\n\"\n",
        "\n",
        "\n",
        "def few_shot(i):\n",
        "    content = test_data['content'].iloc[i]\n",
        "    real_title = test_data['title'].iloc[i]\n",
        "\n",
        "    prompt1 = f\"\"\"The text below is a news content. Your task is to write a short, clear, and relevant Persian title that summarizes the main idea of the article.\n",
        "Generate only one Persian title and do not include any translations or extra text. Two prepared examples are provided below. Study them carefully and write your output in the same style.\n",
        "Examples:\\n\n",
        "{examples_text}\\n\\n\n",
        "News content:\\n\\n{content}\\n\\ntitle:\"\"\"\n",
        "    prompt1 = truncate_prompt(tokenizer, prompt1)\n",
        "    result = pipe(prompt1, max_new_tokens=50, temperature=0.6, do_sample=True)\n",
        "    print(\"Result for prompt 1: \" + clean_output_title(result[0][\"generated_text\"]))\n",
        "\n",
        "    prompt2 = f\"\"\"Write a title in Persian for the news article below.\n",
        "Do not include any English words, translations, or extra explanations.Two prepared examples are provided below. Study them carefully and write your output in the same style.\n",
        "Examples:\\n\n",
        "{examples_text}\\n\\n\n",
        "News content:\\n\\n{content}\\n\\ntitle:\"\"\"\n",
        "\n",
        "    prompt2 = truncate_prompt(tokenizer, prompt2)\n",
        "    result = pipe(prompt2, max_new_tokens=50, temperature=0.6, do_sample=True)\n",
        "    print(\"Result for prompt 2: \" + clean_output_title(result[0][\"generated_text\"]))\n",
        "\n",
        "    prompt3 = f\"\"\"You are a journalist. Your task is to read the news content below and write a single, clear, and concise title **in Persian** that best summarizes the main idea.\n",
        "Do not include any English words, translations, or extra text.Two prepared examples are provided below. Study them carefully and write your output in the same style.\n",
        "Examples:\\n\n",
        "{examples_text}\\n\\n\n",
        "News content:\\n\\n{content}\\n\\ntitle:\"\"\"\n",
        "\n",
        "    prompt3 = truncate_prompt(tokenizer, prompt3)\n",
        "    result = pipe(prompt3, max_new_tokens=50, temperature=0.6, do_sample=True)\n",
        "    print(\"Result for prompt 3: \" + clean_output_title(result[0][\"generated_text\"]))\n",
        "\n",
        "\n",
        "\n",
        "few_shot(0)\n",
        "few_shot(1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4gyqvyIPPxQ"
      },
      "source": [
        "# Fine Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwlVGzwC5EcO"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.0,\n",
        "    bias = \"none\",\n",
        "    random_state=3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-UgevKg7JpW"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def prepare_data(df):\n",
        "    df[\"prompt\"] = df[\"content\"].apply(\n",
        "        lambda x: f\"\"\"### Instruction:\\nGenerate a short and suitable Persian title for the following news content.\n",
        "         Examples for train include input and\n",
        "        what you should output are here: \\n\\n### input:\\n{x}\\n\\n### output:\"\"\"\n",
        "    )\n",
        "    df[\"response\"] = df[\"title\"].apply(lambda x: f\" {x.strip()}\")\n",
        "    return df[[\"prompt\", \"response\"]]\n",
        "\n",
        "\n",
        "train_df = prepare_data(train_data)\n",
        "test_df = prepare_data(test_data)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TxLoKRK7Jsj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPfQ7Qbp7Jvm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from torch.cuda import is_bf16_supported\n",
        "\n",
        "MAX_SEQ_LEN = 2048\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_func(examples):\n",
        "    texts = []\n",
        "    for prompt, response in zip(examples[\"prompt\"], examples[\"response\"]):\n",
        "        texts.append(f\"{prompt.strip()} {response.strip()}{EOS_TOKEN}\")\n",
        "    return texts\n",
        "\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 8,\n",
        "    warmup_steps = 5,\n",
        "    max_steps = 113,\n",
        "    learning_rate = 1e-5,\n",
        "    fp16 = not is_bf16_supported(),\n",
        "    bf16 = is_bf16_supported(),\n",
        "    logging_steps = 1,\n",
        "    output_dir = \"outputs\",\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    save_total_limit = 2,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = test_dataset,\n",
        "    max_seq_length = MAX_SEQ_LEN,\n",
        "    args = training_args,\n",
        "    formatting_func = formatting_func,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xEUj9mYQtT2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_title_finetuned(model, tokenizer, news_text, max_len=50, temperature=0.7):\n",
        "    inputs = tokenizer(news_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_len,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # title = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    title = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return title\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po9uniAK4v8b"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_output(text):\n",
        "    match = re.split(r\"### output:\", text, flags=re.IGNORECASE)\n",
        "    if len(match) > 1:\n",
        "        return match[-1].strip()\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgZvA9QS0Znf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "generated_titles = []\n",
        "\n",
        "for prompt in tqdm(test_df[\"prompt\"], desc=\"Generating titles\"):\n",
        "    prompt = truncate_prompt(tokenizer, prompt)\n",
        "    title = generate_title_finetuned(model, tokenizer, prompt)\n",
        "\n",
        "    if isinstance(title, list):\n",
        "        text = title[0]\n",
        "    else:\n",
        "        text = title\n",
        "\n",
        "    clean_title = clean_output(text)\n",
        "    generated_titles.append(clean_title)\n",
        "\n",
        "test_df[\"generated_title\"] = generated_titles\n",
        "\n",
        "print(test_df[[\"prompt\", \"response\", \"generated_title\"]].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuoCKJ8ae3cP"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vK5Saqxzr9l"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "meteor = evaluate.load(\"meteor\")\n",
        "\n",
        "references = test_df[\"response\"].tolist()\n",
        "predictions = test_df[\"generated_title\"].tolist()\n",
        "\n",
        "bleu_result = bleu.compute(predictions=predictions, references=references)\n",
        "\n",
        "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
        "\n",
        "meteor_result = meteor.compute(predictions=predictions, references=references)\n",
        "\n",
        "print(\"BLEU:\", bleu_result[\"bleu\"])\n",
        "print(\"ROUGE-L:\", rouge_result[\"rougeL\"])\n",
        "print(\"METEOR:\", meteor_result[\"meteor\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NBeLiHRkHZC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2uHiFQ8HDw0"
      },
      "outputs": [],
      "source": [
        "test_df.to_csv(\"/content/gemma_results.csv\", index=False, encoding=\"utf-8-sig\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ncKudOSHMeJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}